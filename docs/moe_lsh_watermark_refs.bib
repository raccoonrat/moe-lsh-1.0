@inproceedings{kirchenbauer2023watermark,
  title={A Watermark for Large Language Models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Moffitt, Micah and Bau, David and Zhu, Fengqing and Goldstein, Tom},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://openreview.net/forum?id=DEJIDCmWOz}
}

@inproceedings{zhao2024semstamp,
  title={SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation},
  author={Zhao, Xuandong and Li, Prabhanjan Ananth and Wang, Yu-Xiang},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year={2024},
  pages={2804--2820},
  url={https://aclanthology.org/2024.naacl-long.226/}
}

@inproceedings{kuditipudi2023unigram,
  title={Robust Watermarking for Machine Translation},
  author={Kuditipudi, Rohith and Thakkar, Adithya and Hashimoto, Tatsunori and Hashimoto, Tatsunori and Jha, Diptikalyan and Jha, Diptikalyan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2306.17439}
}

@inproceedings{qu2024multibit,
  title={Multi-Bit Watermarking for Large Language Models},
  author={Qu, Yihan and Shen, Xinyue and He, Xinlei and Backes, Michael and Zhang, Yang},
  booktitle={USENIX Security Symposium},
  year={2025},
  url={https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-446-qu-watermarking.pdf}
}

@inproceedings{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  volume={35},
  pages={19300--19314},
  url={https://papers.nips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html}
}

@article{zhou2024moe,
  title={Understanding Routing in Mixture-of-Experts Models},
  author={Zhou, Yanqi and Wang, Nan and Roy, Abhishek and Vaidya, Amar and Khodak, Mikhail and Saenko, Kate and Guyon, Isabelle},
  journal={arXiv preprint arXiv:2402.01739},
  year={2024},
  url={https://arxiv.org/abs/2402.01739}
}

@article{wang2024waterpark,
  title={WaterPark: A Unified Evaluation Framework for Watermarking Large Language Models},
  author={Wang, Yihan and Shen, Xinyue and He, Xinlei and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2411.13425},
  year={2024},
  url={https://arxiv.org/html/2411.13425v3}
}

@inproceedings{liu2025b4,
  title={B4: A Black-Box Watermark Removal Attack},
  author={Liu, Yuxin and Li, Yihan and Wang, Yu-Xiang},
  booktitle={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year={2025},
  url={https://aclanthology.org/2025.naacl-long.460/}
}

@article{charlesworth2024attacking,
  title={Attacking Watermarking by Exploiting Strengths},
  author={Charlesworth, Andrew and others},
  journal={arXiv preprint arXiv:2402.16187},
  year={2024},
  url={https://arxiv.org/html/2402.16187v1}
}

@inproceedings{raffel2020t5,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{lewis2019bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}

@article{raganato2020webnlg,
  title={The WebNLG Challenge: Generating Text from RDF Data},
  author={Raganato, Alessandro and Bontcheva, Kalina and Derczynski, Leon},
  journal={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={124--133},
  year={2017}
}

@article{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Reading Comprehension},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016}
}

@article{kwiatkowski2019natural,
  title={Natural Questions: A Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom and Palomaki, Jennimae and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019}
}

@article{see2017get,
  title={Get to the Point: Summarization with Pointer-Generator Networks},
  author={See, Abigail and Liu, Peter J and Manning, Christopher D},
  journal={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
  pages={1073--1083},
  year={2017}
}

@article{narayan2018don,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1797--1807},
  year={2018}
}

@misc{mixtral2024,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  year={2024},
  url={https://mistral.ai/news/mixtral-of-experts/}
}

@misc{openmoe2024,
  title={OpenMoE: An Open Mixture-of-Experts Framework},
  author={Xue, Fuzhao and Wei, Xiaozhe and Chen, Yuxiang and Liu, Xin and Shi, Chuanlong and Ma, Xiaoxin and Chen, Haipeng and Chen, Zangwei and Huang, Yang You},
  journal={arXiv preprint arXiv:2402.01739},
  year={2024},
  url={https://github.com/XueFuzhao/OpenMoE}
}

@misc{mistral32025,
  title={Mistral 3: Accelerated Open Models},
  author={Mistral AI},
  year={2025},
  url={https://developer.nvidia.com/blog/nvidia-accelerated-mistral-3-open-models-deliver-efficiency-accuracy-at-any-scale/}
}

